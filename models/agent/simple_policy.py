"""
Simple feedforward policy network for token compression.

This replaces the over-engineered transformer-based policy with a straightforward
feedforward network that uses local context windows. Based on the principle that
token compression decisions are local and don't need global self-attention.

Key improvements:
- ~500K parameters instead of ~100M
- O(n) complexity instead of O(nÂ²)
- Local context windows instead of full sequence attention
- Simple architecture that's easy to debug and understand
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Dict, Any
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SimpleCompressionPolicy(nn.Module):
    """
    Simple feedforward policy for token compression decisions.
    
    Uses local context windows around each token to make keep/mask decisions.
    Much simpler than transformer-based approaches but should work just as well
    for this specific task.
    """
    
    def __init__(
        self,
        embedding_dim: int = 768,
        context_window: int = 5,
        hidden_dim: int = 256,
        dropout: float = 0.1,
        device: str = "cuda" if torch.cuda.is_available() else "cpu"
    ):
        """
        Initialize the simple policy network.
        
        Args:
            embedding_dim: Dimension of token embeddings (usually 768 for GPT-2)
            context_window: Size of local context window (should be odd)
            hidden_dim: Hidden layer dimension
            dropout: Dropout probability
            device: Device to run on
        """
        super().__init__()
        
        self.embedding_dim = embedding_dim
        self.context_window = context_window
        self.hidden_dim = hidden_dim
        self.device = device
        
        # Ensure context window is odd for symmetric padding
        if context_window % 2 == 0:
            logger.warning(f"Context window {context_window} is even, using {context_window + 1}")
            self.context_window = context_window + 1
        
        # Input dimension is embedding_dim * context_window
        input_dim = embedding_dim * self.context_window
        
        # Simple feedforward network - no fancy attention, just MLP
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()  # Output probability of keeping token
        )
        
        # Count parameters for sanity check
        param_count = sum(p.numel() for p in self.parameters())
        logger.info(f"Simple policy initialized with {param_count:,} parameters")
        
        if param_count > 1_000_000:
            logger.warning(f"Policy has {param_count:,} parameters - might be too complex")
    
    def forward(self, embeddings: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through the policy network.
        
        Args:
            embeddings: Token embeddings of shape (batch_size, seq_len, embedding_dim)
            
        Returns:
            Keep probabilities of shape (batch_size, seq_len)
        """
        batch_size, seq_len, embed_dim = embeddings.shape
        
        if embed_dim != self.embedding_dim:
            raise ValueError(f"Expected embedding_dim {self.embedding_dim}, got {embed_dim}")
        
        # Pad sequence for context windows
        pad_size = self.context_window // 2
        
        # Use reflection padding to avoid introducing new tokens
        padded = F.pad(embeddings, (0, 0, pad_size, pad_size), mode='reflect')
        
        # Extract context windows for each token
        contexts = []
        for i in range(seq_len):
            window_start = i
            window_end = i + self.context_window
            window = padded[:, window_start:window_end, :].flatten(1)  # Flatten context
            contexts.append(window)
        
        # Stack all contexts: [batch, seq_len, context_dim]
        contexts = torch.stack(contexts, dim=1)
        
        # Pass through network to get keep probabilities
        keep_probs = self.network(contexts).squeeze(-1)  # Remove last dimension
        
        return keep_probs
    
    def get_decisions(
        self, 
        embeddings: torch.Tensor, 
        threshold: float = 0.5,
        temperature: float = 1.0
    ) -> torch.Tensor:
        """
        Get binary keep/mask decisions from the policy.
        
        Args:
            embeddings: Token embeddings
            threshold: Decision threshold (0.5 = balanced)
            temperature: Temperature for softening decisions during training
            
        Returns:
            Binary decisions (1 = keep, 0 = mask)
        """
        logits = self.get_logits(embeddings)
        
        if self.training and temperature != 1.0:
            # Apply temperature scaling during training
            probs = torch.sigmoid(logits / temperature)
        else:
            probs = torch.sigmoid(logits)
        
        return (probs > threshold).float()
    
    def get_logits(self, embeddings: torch.Tensor) -> torch.Tensor:
        """
        Get raw logits before sigmoid activation.
        Useful for computing losses and applying temperature scaling.
        
        Args:
            embeddings: Token embeddings
            
        Returns:
            Raw logits before sigmoid
        """
        # Get the linear output before sigmoid
        batch_size, seq_len, embed_dim = embeddings.shape
        pad_size = self.context_window // 2
        
        padded = F.pad(embeddings, (0, 0, pad_size, pad_size), mode='reflect')
        
        contexts = []
        for i in range(seq_len):
            window_start = i
            window_end = i + self.context_window
            window = padded[:, window_start:window_end, :].flatten(1)
            contexts.append(window)
        
        contexts = torch.stack(contexts, dim=1)
        
        # Get output from all layers except the final sigmoid
        x = contexts
        for layer in self.network[:-1]:  # All layers except sigmoid
            x = layer(x)
        
        return x.squeeze(-1)
    
    def get_context_importance(self, embeddings: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Analyze which parts of the context are most important for decisions.
        Useful for debugging and understanding the model.
        
        Args:
            embeddings: Token embeddings
            
        Returns:
            Dictionary with importance analysis
        """
        batch_size, seq_len, embed_dim = embeddings.shape
        
        # Get keep probabilities
        keep_probs = self.forward(embeddings)
        
        # Simple importance measure: gradient of output w.r.t. input
        embeddings_copy = embeddings.detach().clone().requires_grad_(True)
        keep_probs_grad = self.forward(embeddings_copy)
        loss = keep_probs_grad.sum()
        loss.backward(retain_graph=True)
        
        input_importance = torch.abs(embeddings_copy.grad).mean(dim=-1) if embeddings_copy.grad is not None else torch.zeros(embeddings_copy.shape[:2])
        
        return {
            'keep_probs': keep_probs,
            'input_importance': input_importance,
            'mean_keep_prob': keep_probs.mean(),
            'compression_ratio': 1.0 - keep_probs.mean()
        }


class SimplePolicyLoss(nn.Module):
    """
    Simple loss function for training the policy network.
    Can be used for supervised learning or as part of RL training.
    """
    
    def __init__(self, entropy_weight: float = 0.01):
        """
        Initialize policy loss.
        
        Args:
            entropy_weight: Weight for entropy regularization
        """
        super().__init__()
        self.entropy_weight = entropy_weight
    
    def forward(
        self,
        logits: torch.Tensor,
        targets: Optional[torch.Tensor] = None,
        rewards: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Compute policy loss.
        
        Args:
            logits: Raw logits from policy network
            targets: Target decisions for supervised learning (optional)
            rewards: Rewards for each decision for RL (optional)
            
        Returns:
            Dictionary with loss components
        """
        probs = torch.sigmoid(logits)
        
        # Entropy for regularization
        entropy = -(probs * torch.log(probs + 1e-8) + 
                   (1 - probs) * torch.log(1 - probs + 1e-8)).mean()
        
        losses = {'entropy': entropy}
        
        if targets is not None:
            # Supervised learning loss
            bce_loss = F.binary_cross_entropy(probs, targets.float())
            total_loss = bce_loss - self.entropy_weight * entropy
            
            losses.update({
                'bce_loss': bce_loss,
                'total_loss': total_loss
            })
        
        if rewards is not None:
            # Policy gradient loss (REINFORCE)
            log_probs = torch.log(probs + 1e-8)
            policy_loss = -(log_probs * rewards).mean()
            total_loss = policy_loss - self.entropy_weight * entropy
            
            losses.update({
                'policy_loss': policy_loss,
                'total_loss': total_loss
            })
        
        return losses


# Utility functions for creating and testing the policy

def create_simple_policy(config: Dict[str, Any]) -> SimpleCompressionPolicy:
    """Create a simple policy from configuration."""
    return SimpleCompressionPolicy(**config)


def test_simple_policy():
    """Basic functionality test for the simple policy."""
    logger.info("Testing simple policy...")
    
    # Create test data
    batch_size, seq_len, embed_dim = 2, 10, 768
    embeddings = torch.randn(batch_size, seq_len, embed_dim)
    
    # Create policy
    policy = SimpleCompressionPolicy(embedding_dim=embed_dim, context_window=5)
    
    # Test forward pass
    keep_probs = policy(embeddings)
    assert keep_probs.shape == (batch_size, seq_len), f"Expected shape {(batch_size, seq_len)}, got {keep_probs.shape}"
    assert torch.all((keep_probs >= 0) & (keep_probs <= 1)), "Keep probabilities should be in [0, 1]"
    
    # Test decisions
    decisions = policy.get_decisions(embeddings)
    assert torch.all((decisions == 0) | (decisions == 1)), "Decisions should be binary"
    
    # Test logits
    logits = policy.get_logits(embeddings)
    assert logits.shape == (batch_size, seq_len), "Logits shape mismatch"
    
    # Test context importance
    importance = policy.get_context_importance(embeddings)
    assert 'keep_probs' in importance, "Missing keep_probs in importance analysis"
    
    logger.info("Simple policy tests passed!")
    
    # Print some statistics
    param_count = sum(p.numel() for p in policy.parameters())
    logger.info(f"Policy has {param_count:,} parameters")
    logger.info(f"Mean keep probability: {keep_probs.mean():.3f}")
    logger.info(f"Compression ratio: {1.0 - keep_probs.mean():.3f}")


if __name__ == "__main__":
    test_simple_policy()